Machine Learning and AI Beyond the Basics Book
The Supplementary Materials for the Machine Learning Q and AI book by Sebastian Raschka.

Please use the Discussions for any questions about the book!

2023-ml-qai-cover


About the Book
If you’ve locked down the basics of machine learning and AI and want a fun way to address lingering knowledge gaps, this book is for you. This rapid-fire series of short chapters addresses 30 essential questions in the field, helping you stay current on the latest technologies you can implement in your own work.

Each chapter of Machine Learning Q and AI asks and answers a central question, with diagrams to explain new concepts and ample references for further reading

Multi-GPU training paradigms
Finetuning transformers
Differences between encoder- and decoder-style LLMs
Concepts behind vision transformers
Confidence intervals for ML
And many more!
This book is a fully edited and revised version of Machine Learning Q and AI, which was available on Leanpub.


Reviews
“One could hardly ask for a better guide than Sebastian, who is, without exaggeration, the best machine learning educator currently in the field. On each page, Sebastian not only imparts his extensive knowledge but also shares the passion and curiosity that mark true expertise.”
-- Chris Albon, Director of Machine Learning, The Wikimedia Foundation


Links
Preorder directly from No Starch press
Preorder directly from Amazon
Supplementary Materias and Discussions


Table of Contents
Title	URL Link	Supplementary Code
1	Embeddings, Representations, and Latent Space	
2	Self-Supervised Learning	
3	Few-Shot Learning	
4	The Lottery Ticket Hypothesis	
5	Reducing Overfitting with Data	
6	Reducing Overfitting with Model Modifications	
7	Multi-GPU Training Paradigms	
8	The Keys to the Success of Transformers	
9	Generative AI Models	
10	Sources of Randomness	data-sampling.ipynb
dropout.ipynb
random-weights.ipynb
PART II: COMPUTER VISION	
11	Calculating the Number of Parameters	conv-size.ipynb
12	The Equivalence of Fully Connected and Convolutional Layers	fc-cnn-equivalence.ipynb
13	Large Training Sets for Vision Transformers	
PART III: NATURAL LANGUAGE PROCESSING	
14	The Distributional Hypothesis	
15	Data Augmentation for Text	backtranslation.ipynb
noise-injection.ipynb
sentence-order-shuffling.ipynb
synonym-replacement.ipynb
synthetic-data.ipynb
word-deletion.ipynb
word-position-swapping.ipynb
16	“Self”-Attention	
17	Encoder- And Decoder-Style Transformers	
18	Using and Finetuning Pretrained Transformers	
19	Evaluating Generative Large Language Models	BERTScore.ipynb
bleu.ipynb
perplexity.ipynb
rouge.ipynb
PART IV: PRODUCTION AND DEPLOYMENT	
20	Stateless And Stateful Training	
21	Data-Centric AI	
22	Speeding Up Inference	
23	Data Distribution Shifts	
PART V: PREDICTIVE PERFORMANCE AND MODEL EVALUATION	
24	Poisson and Ordinal Regression	
25	Confidence Intervals	four-methods.ipynb
four-methods-vs-true-value.ipynb
26	Confidence Intervals Versus Conformal Predictions	conformal_prediction.ipynb
27	Proper Metrics	
28	The K in K-Fold Cross-Validation	
29	Training and Test Set Discordance	
30	Limited Labeled Data
